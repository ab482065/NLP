{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e46ba26",
   "metadata": {},
   "source": [
    "# Word Count Input Statistical Analysis for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "22c462db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import emoji\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a5214cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cf3e9",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6610e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    return emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "def slice_dataframe_and_compute_word_frequency(df, slice_cols, slice_vals, text_col, spacy_lang_pkg):\n",
    "    sliced_df = df.copy()\n",
    "    for i in range(len(slice_cols)):\n",
    "        sliced_df = sliced_df[sliced_df[slice_cols[i]] == slice_vals[i]]\n",
    "    print(f'Found a total of {len(sliced_df)} examples')\n",
    "    nlp = spacy.load(spacy_lang_pkg)\n",
    "    text = ' '.join(sliced_df[text_col])\n",
    "    text = emoji.get_emoji_regexp().sub(r'', text)\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc if not token.is_stop and not token.is_punct and len(token) > 1]\n",
    "    freqs = Counter(words)\n",
    "    pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "    for k,v in sorted(pos_counts.items()):\n",
    "        print(f'{k:{4}}. {doc.vocab[k].text:{5}}: {v}')\n",
    "    return freqs\n",
    "\n",
    "def slice_dataframe_and_compute_pos_tags(df, slice_cols, slice_vals, text_col, spacy_lang_pkg):\n",
    "    sliced_df = df.copy()\n",
    "    for i in range(len(slice_cols)):\n",
    "        sliced_df = sliced_df[sliced_df[slice_cols[i]] == slice_vals[i]]\n",
    "    print(f'Found a total of {len(sliced_df)} examples')\n",
    "    nlp = spacy.load(spacy_lang_pkg)\n",
    "    text = ' '.join(sliced_df[text_col])\n",
    "    text = emoji.get_emoji_regexp().sub(r'', text)\n",
    "    doc = nlp(text)\n",
    "    pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "    for k,v in sorted(pos_counts.items()):\n",
    "        print(f'{k:{4}}. {doc.vocab[k].text:{5}}: {v}')\n",
    "    return pos_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf033b",
   "metadata": {},
   "source": [
    "### Spanish (Basile et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f0fc855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{DATA_DIR}/spanish-basile/hateval2019_es_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8dfb4736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>Easyjet quiere duplicar el número de mujeres p...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20002</td>\n",
       "      <td>El gobierno debe crear un control estricto de ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20003</td>\n",
       "      <td>Yo veo a mujeres destruidas por acoso laboral ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20004</td>\n",
       "      <td>— Yo soy respetuoso con los demás, sólamente l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20007</td>\n",
       "      <td>Antonio Caballero y como ser de mal gusto e ig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  HS  TR  AG\n",
       "0  20001  Easyjet quiere duplicar el número de mujeres p...   1   0   0\n",
       "1  20002  El gobierno debe crear un control estricto de ...   1   0   0\n",
       "2  20003  Yo veo a mujeres destruidas por acoso laboral ...   0   0   0\n",
       "3  20004  — Yo soy respetuoso con los demás, sólamente l...   0   0   0\n",
       "4  20007  Antonio Caballero y como ser de mal gusto e ig...   0   0   0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0468b6",
   "metadata": {},
   "source": [
    "#### 1.1 Word Frequecy for Examples that are Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e3d13a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 1857 examples\n",
      "  84. ADJ  : 2986\n",
      "  85. ADP  : 4253\n",
      "  86. ADV  : 1742\n",
      "  87. AUX  : 1624\n",
      "  89. CCONJ: 1452\n",
      "  90. DET  : 4246\n",
      "  91. INTJ : 40\n",
      "  92. NOUN : 7060\n",
      "  93. NUM  : 389\n",
      "  94. PART : 10\n",
      "  95. PRON : 3129\n",
      "  96. PROPN: 4764\n",
      "  97. PUNCT: 4489\n",
      "  98. SCONJ: 1171\n",
      "  99. SYM  : 140\n",
      " 100. VERB : 4512\n",
      " 103. SPACE: 359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('puta', 418),\n",
       " ('perra', 345),\n",
       " ('zorra', 216),\n",
       " ('Cállate', 122),\n",
       " ('callate', 110),\n",
       " ('inmigrantes', 102),\n",
       " ('mujer', 96),\n",
       " ('Callate', 91),\n",
       " ('mereces', 82),\n",
       " ('cállate', 78)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['HS'], [1], 'text', 'es_core_news_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0220d",
   "metadata": {},
   "source": [
    "#### 1.2 Word Frequecy for Examples that are Hate Speech and Aggressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ee99b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 1502 examples\n",
      "  84. ADJ  : 2369\n",
      "  85. ADP  : 3153\n",
      "  86. ADV  : 1342\n",
      "  87. AUX  : 1256\n",
      "  89. CCONJ: 1126\n",
      "  90. DET  : 3133\n",
      "  91. INTJ : 29\n",
      "  92. NOUN : 5371\n",
      "  93. NUM  : 263\n",
      "  94. PART : 9\n",
      "  95. PRON : 2457\n",
      "  96. PROPN: 4005\n",
      "  97. PUNCT: 3335\n",
      "  98. SCONJ: 861\n",
      "  99. SYM  : 114\n",
      " 100. VERB : 3486\n",
      " 103. SPACE: 287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('puta', 398),\n",
       " ('perra', 322),\n",
       " ('zorra', 207),\n",
       " ('Cállate', 122),\n",
       " ('callate', 109),\n",
       " ('Callate', 90),\n",
       " ('mereces', 80),\n",
       " ('cállate', 78),\n",
       " ('mierda', 74),\n",
       " ('PUTA', 67)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['HS', 'AG'], [1, 1], 'text', 'es_core_news_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6586a29",
   "metadata": {},
   "source": [
    "#### 1.3 Word Frequecy for Examples that are Hate Speech but not Aggressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "539b5519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 355 examples\n",
      "  84. ADJ  : 618\n",
      "  85. ADP  : 1105\n",
      "  86. ADV  : 396\n",
      "  87. AUX  : 370\n",
      "  89. CCONJ: 325\n",
      "  90. DET  : 1114\n",
      "  91. INTJ : 13\n",
      "  92. NOUN : 1687\n",
      "  93. NUM  : 122\n",
      "  95. PRON : 666\n",
      "  96. PROPN: 773\n",
      "  97. PUNCT: 1149\n",
      "  98. SCONJ: 318\n",
      "  99. SYM  : 27\n",
      " 100. VERB : 1015\n",
      " 103. SPACE: 72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mujer', 54),\n",
       " ('inmigrantes', 36),\n",
       " ('mujeres', 32),\n",
       " ('subsaharianos', 23),\n",
       " ('perra', 23),\n",
       " ('puta', 20),\n",
       " ('España', 17),\n",
       " ('país', 17),\n",
       " ('papeles', 16),\n",
       " ('inmigrante', 15)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['HS', 'AG'], [1, 0], 'text', 'es_core_news_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a1140",
   "metadata": {},
   "source": [
    "#### 2.  Word Frequecy for Examples that are not Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4fcd9222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 2643 examples\n",
      "  84. ADJ  : 4239\n",
      "  85. ADP  : 7222\n",
      "  86. ADV  : 2678\n",
      "  87. AUX  : 2382\n",
      "  89. CCONJ: 2147\n",
      "  90. DET  : 6409\n",
      "  91. INTJ : 118\n",
      "  92. NOUN : 11411\n",
      "  93. NUM  : 733\n",
      "  94. PART : 10\n",
      "  95. PRON : 4591\n",
      "  96. PROPN: 7449\n",
      "  97. PUNCT: 6388\n",
      "  98. SCONJ: 1819\n",
      "  99. SYM  : 352\n",
      " 100. VERB : 6922\n",
      " 103. SPACE: 506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('puta', 784),\n",
       " ('hijo', 237),\n",
       " ('acoso', 198),\n",
       " ('madre', 149),\n",
       " ('perra', 142),\n",
       " ('polla', 141),\n",
       " ('PUTA', 139),\n",
       " ('mierda', 117),\n",
       " ('mereces', 112),\n",
       " ('violación', 110)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['HS'], [0], 'text', 'es_core_news_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f0ff63",
   "metadata": {},
   "source": [
    "### Spanish (Pereira et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "adc8835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary wrangling to fit it into a DataFrame\n",
    "data = []\n",
    "with open(f'{DATA_DIR}/spanish-pereira/labeled_corpus_6K.txt', 'r',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        curr = line.split(\";||;\")\n",
    "        data.append(curr[:-1] + [int(curr[-1][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3f0665c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=np.array(data), columns=['id', 'text', 'HS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "84386282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HS'] = df.HS.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f54bb2c",
   "metadata": {},
   "source": [
    "#### 1. Word Frequecy for Examples that are Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dae2d123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 1567 examples\n",
      "  84. ADJ  : 2410\n",
      "  85. ADP  : 3006\n",
      "  86. ADV  : 1390\n",
      "  87. AUX  : 1427\n",
      "  89. CCONJ: 999\n",
      "  90. DET  : 2773\n",
      "  91. INTJ : 52\n",
      "  92. NOUN : 4982\n",
      "  93. NUM  : 238\n",
      "  94. PART : 4\n",
      "  95. PRON : 2119\n",
      "  96. PROPN: 3030\n",
      "  97. PUNCT: 3351\n",
      "  98. SCONJ: 999\n",
      "  99. SYM  : 61\n",
      " 100. VERB : 3164\n",
      " 103. SPACE: 242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('subnormal', 350),\n",
       " ('mierda', 144),\n",
       " ('puto', 97),\n",
       " ('puta', 91),\n",
       " ('fascista', 85),\n",
       " ('fachas', 82),\n",
       " ('facha', 66),\n",
       " ('fascistas', 61),\n",
       " ('nazi', 44),\n",
       " ('retrasado', 42)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['HS'], [1], 'text', 'es_core_news_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9910ad7f",
   "metadata": {},
   "source": [
    "#### 2. Word Frequecy for Examples that are not Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0bfa6b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 4433 examples\n",
      "  84. ADJ  : 6948\n",
      "  85. ADP  : 9660\n",
      "  86. ADV  : 4262\n",
      "  87. AUX  : 4655\n",
      "  89. CCONJ: 3070\n",
      "  90. DET  : 9212\n",
      "  91. INTJ : 145\n",
      "  92. NOUN : 15314\n",
      "  93. NUM  : 1058\n",
      "  94. PART : 14\n",
      "  95. PRON : 5951\n",
      "  96. PROPN: 9354\n",
      "  97. PUNCT: 10326\n",
      "  98. SCONJ: 2971\n",
      "  99. SYM  : 259\n",
      " 100. VERB : 9312\n",
      " 103. SPACE: 787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('independentista', 501),\n",
       " ('subnormal', 306),\n",
       " ('fascista', 200),\n",
       " ('facha', 196),\n",
       " ('fascistas', 157),\n",
       " ('negro', 145),\n",
       " ('fachas', 135),\n",
       " ('mayoría', 124),\n",
       " ('género', 121),\n",
       " ('gente', 118)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['HS'], [0], 'text', 'es_core_news_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfeb074",
   "metadata": {},
   "source": [
    "### French (Ousidhoum et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e2b4dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{DATA_DIR}/french-ousidhoum/fr_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e97aefa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HITId</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>directness</th>\n",
       "      <th>annotator_sentiment</th>\n",
       "      <th>target</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rt @user personnage ainsi q tte la clique gauc...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>direct</td>\n",
       "      <td>shock</td>\n",
       "      <td>origin</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@user @user @user bah oui t'as raison l'autre ...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>indirect</td>\n",
       "      <td>shock</td>\n",
       "      <td>other</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moi j'ai personne à qui m'adresser pour réclam...</td>\n",
       "      <td>hateful</td>\n",
       "      <td>direct</td>\n",
       "      <td>anger_confusion_sadness_fear_disgust</td>\n",
       "      <td>other</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@user @user c...est vrai que les anchois sont ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>indirect</td>\n",
       "      <td>indifference</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>eh les renois faut se réveiller la @url</td>\n",
       "      <td>normal</td>\n",
       "      <td>direct</td>\n",
       "      <td>sadness</td>\n",
       "      <td>origin</td>\n",
       "      <td>african_descent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HITId                                              tweet  sentiment  \\\n",
       "0      1  rt @user personnage ainsi q tte la clique gauc...    abusive   \n",
       "1      2  @user @user @user bah oui t'as raison l'autre ...  offensive   \n",
       "2      3  moi j'ai personne à qui m'adresser pour réclam...    hateful   \n",
       "3      4  @user @user c...est vrai que les anchois sont ...     normal   \n",
       "4      5            eh les renois faut se réveiller la @url     normal   \n",
       "\n",
       "  directness                   annotator_sentiment  target            group  \n",
       "0     direct                                 shock  origin            other  \n",
       "1   indirect                                 shock   other       individual  \n",
       "2     direct  anger_confusion_sadness_fear_disgust   other            women  \n",
       "3   indirect                          indifference   other            other  \n",
       "4     direct                               sadness  origin  african_descent  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aef7e2",
   "metadata": {},
   "source": [
    "#### 1. Word Frequency for Examples that are not Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "730a3ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 821 examples\n",
      "  84. ADJ  : 1311\n",
      "  85. ADP  : 1596\n",
      "  86. ADV  : 1041\n",
      "  87. AUX  : 567\n",
      "  89. CCONJ: 373\n",
      "  90. DET  : 1541\n",
      "  92. NOUN : 2926\n",
      "  93. NUM  : 98\n",
      "  95. PRON : 1500\n",
      "  96. PROPN: 276\n",
      "  97. PUNCT: 699\n",
      "  98. SCONJ: 306\n",
      "  99. SYM  : 3\n",
      " 100. VERB : 2573\n",
      " 101. X    : 9\n",
      " 103. SPACE: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('@user', 970),\n",
       " ('@url', 475),\n",
       " ('gauchiste', 93),\n",
       " ('renois', 79),\n",
       " ('attarde', 66),\n",
       " ('migrants', 62),\n",
       " ('violence', 60),\n",
       " ('contre', 53),\n",
       " ('arabes', 43),\n",
       " ('attardé', 42)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['normal'], 'tweet', 'fr_core_news_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad692cc",
   "metadata": {},
   "source": [
    "#### 2. Word Frequency for Examples that are Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0ab69fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 594 examples\n",
      "  84. ADJ  : 1088\n",
      "  85. ADP  : 1147\n",
      "  86. ADV  : 497\n",
      "  87. AUX  : 349\n",
      "  89. CCONJ: 221\n",
      "  90. DET  : 1262\n",
      "  92. NOUN : 2142\n",
      "  93. NUM  : 87\n",
      "  95. PRON : 911\n",
      "  96. PROPN: 182\n",
      "  97. PUNCT: 492\n",
      "  98. SCONJ: 141\n",
      "  99. SYM  : 3\n",
      " 100. VERB : 1367\n",
      " 101. X    : 13\n",
      " 103. SPACE: 43\n",
      "Found a total of 1336 examples\n",
      "  84. ADJ  : 2016\n",
      "  85. ADP  : 1976\n",
      "  86. ADV  : 1166\n",
      "  87. AUX  : 851\n",
      "  89. CCONJ: 482\n",
      "  90. DET  : 2152\n",
      "  91. INTJ : 2\n",
      "  92. NOUN : 3689\n",
      "  93. NUM  : 195\n",
      "  95. PRON : 2027\n",
      "  96. PROPN: 387\n",
      "  97. PUNCT: 906\n",
      "  98. SCONJ: 429\n",
      "  99. SYM  : 4\n",
      " 100. VERB : 4465\n",
      " 101. X    : 16\n",
      " 103. SPACE: 64\n",
      "Found a total of 207 examples\n",
      "  84. ADJ  : 389\n",
      "  85. ADP  : 375\n",
      "  86. ADV  : 201\n",
      "  87. AUX  : 130\n",
      "  89. CCONJ: 91\n",
      "  90. DET  : 440\n",
      "  92. NOUN : 761\n",
      "  93. NUM  : 23\n",
      "  95. PRON : 349\n",
      "  96. PROPN: 76\n",
      "  97. PUNCT: 185\n",
      "  98. SCONJ: 54\n",
      " 100. VERB : 554\n",
      " 101. X    : 5\n",
      " 103. SPACE: 24\n",
      "Found a total of 142 examples\n",
      "  84. ADJ  : 269\n",
      "  85. ADP  : 266\n",
      "  86. ADV  : 135\n",
      "  87. AUX  : 98\n",
      "  89. CCONJ: 57\n",
      "  90. DET  : 299\n",
      "  92. NOUN : 548\n",
      "  93. NUM  : 21\n",
      "  95. PRON : 256\n",
      "  96. PROPN: 38\n",
      "  97. PUNCT: 109\n",
      "  98. SCONJ: 51\n",
      " 100. VERB : 367\n",
      " 103. SPACE: 5\n",
      "Found a total of 236 examples\n",
      "  84. ADJ  : 435\n",
      "  85. ADP  : 392\n",
      "  86. ADV  : 209\n",
      "  87. AUX  : 175\n",
      "  89. CCONJ: 73\n",
      "  90. DET  : 468\n",
      "  92. NOUN : 818\n",
      "  93. NUM  : 39\n",
      "  95. PRON : 389\n",
      "  96. PROPN: 97\n",
      "  97. PUNCT: 213\n",
      "  98. SCONJ: 84\n",
      " 100. VERB : 566\n",
      " 101. X    : 6\n",
      " 103. SPACE: 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('@user', 2335),\n",
       " ('@url', 1104),\n",
       " ('mongol', 517),\n",
       " ('attardé', 379),\n",
       " ('gauchiste', 312),\n",
       " ('renois', 253),\n",
       " ('sale', 228),\n",
       " ('rebeus', 188),\n",
       " ('arabe', 133),\n",
       " ('migrants', 105)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['abusive'], 'tweet', 'fr_core_news_sm')\n",
    "freqs += slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['offensive'], 'tweet', 'fr_core_news_sm')\n",
    "freqs += slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['hateful'], 'tweet', 'fr_core_news_sm')\n",
    "freqs += slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['disrespectful'], 'tweet', 'fr_core_news_sm')\n",
    "freqs += slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['fearful'], 'tweet', 'fr_core_news_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e5e994",
   "metadata": {},
   "source": [
    "### Turkish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "82e3ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{DATA_DIR}/turkish/offenseval-tr-training-v1/offenseval-tr-training-v1.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b1c451f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20948</td>\n",
       "      <td>@USER en güzel uyuyan insan ödülü jeon jungkoo...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10134</td>\n",
       "      <td>@USER Mekanı cennet olsun, saygılar sayın avuk...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23457</td>\n",
       "      <td>Kızlar aranızda kas yığını beylere düşenler ol...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18401</td>\n",
       "      <td>Biraz ders çalışayım. Tembellik ve uyku düşman...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17525</td>\n",
       "      <td>@USER Trezeguet yerine El Sharawy daha iyi olm...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a\n",
       "0  20948  @USER en güzel uyuyan insan ödülü jeon jungkoo...       NOT\n",
       "1  10134  @USER Mekanı cennet olsun, saygılar sayın avuk...       NOT\n",
       "2  23457  Kızlar aranızda kas yığını beylere düşenler ol...       NOT\n",
       "3  18401  Biraz ders çalışayım. Tembellik ve uyku düşman...       NOT\n",
       "4  17525  @USER Trezeguet yerine El Sharawy daha iyi olm...       NOT"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb2a42",
   "metadata": {},
   "source": [
    "#### 1. Word Frequency for tweets that are Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f11b5978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 6046 examples\n",
      "  84. ADJ  : 6137\n",
      "  85. ADP  : 1100\n",
      "  86. ADV  : 962\n",
      "  87. AUX  : 479\n",
      "  89. CCONJ: 46\n",
      "  90. DET  : 184\n",
      "  91. INTJ : 742\n",
      "  92. NOUN : 26053\n",
      "  93. NUM  : 1753\n",
      "  94. PART : 260\n",
      "  95. PRON : 571\n",
      "  96. PROPN: 67725\n",
      "  97. PUNCT: 11159\n",
      "  98. SCONJ: 4\n",
      "  99. SYM  : 820\n",
      " 100. VERB : 6404\n",
      " 101. X    : 2283\n",
      " 103. SPACE: 4165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('@USER', 5367),\n",
       " ('  ', 1380),\n",
       " ('bir', 1267),\n",
       " ('bu', 1167),\n",
       " ('ve', 847),\n",
       " ('ne', 718),\n",
       " ('de', 688),\n",
       " ('da', 660),\n",
       " ('için', 546),\n",
       " ('gibi', 528)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['subtask_a'], ['OFF'], 'tweet', 'en_core_web_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7bf92",
   "metadata": {},
   "source": [
    "#### 2. Word Frequency for tweets that are not Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0380031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is too long for Spacy. DON'T RUN\n",
    "# freqs = slice_dataframe_and_compute_word_frequency(df, ['subtask_a'], ['NOT'], 'tweet', 'en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9f81b",
   "metadata": {},
   "source": [
    "### Danish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "95495c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{DATA_DIR}/danish/data/offenseval-da-training-v1.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "14838f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3131</td>\n",
       "      <td>Jeg tror det vil være dejlig køligt, men jeg v...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>711</td>\n",
       "      <td>Så kommer de nok til at investere i en ny cyke...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2500</td>\n",
       "      <td>Nu er det jo også de Ikea-aber der har lavet s...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2678</td>\n",
       "      <td>128 Varme emails, er vi enige om at det er sex...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>784</td>\n",
       "      <td>Desværre tyder det på, at amerikanerne er helt...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              tweet subtask_a\n",
       "0  3131  Jeg tror det vil være dejlig køligt, men jeg v...       NOT\n",
       "1   711  Så kommer de nok til at investere i en ny cyke...       NOT\n",
       "2  2500  Nu er det jo også de Ikea-aber der har lavet s...       OFF\n",
       "3  2678  128 Varme emails, er vi enige om at det er sex...       NOT\n",
       "4   784  Desværre tyder det på, at amerikanerne er helt...       NOT"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d1771",
   "metadata": {},
   "source": [
    "#### 1. Word Frequency for tweets that are Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ab80da3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 384 examples\n",
      "  84. ADJ  : 763\n",
      "  85. ADP  : 1057\n",
      "  86. ADV  : 1286\n",
      "  87. AUX  : 728\n",
      "  89. CCONJ: 295\n",
      "  90. DET  : 639\n",
      "  91. INTJ : 24\n",
      "  92. NOUN : 1816\n",
      "  93. NUM  : 70\n",
      "  94. PART : 154\n",
      "  95. PRON : 1160\n",
      "  96. PROPN: 446\n",
      "  97. PUNCT: 1279\n",
      "  98. SCONJ: 213\n",
      "  99. SYM  : 25\n",
      " 100. VERB : 1246\n",
      " 101. X    : 86\n",
      " 103. SPACE: 135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lort', 45),\n",
       " ('@USER', 31),\n",
       " ('bare', 29),\n",
       " ('  ', 24),\n",
       " ('godt', 22),\n",
       " ('fandme', 22),\n",
       " ('når', 19),\n",
       " ('folk', 19),\n",
       " ('Fuck', 18),\n",
       " ('lortet', 15)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['subtask_a'], ['OFF'], 'tweet', 'da_core_news_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e3f59",
   "metadata": {},
   "source": [
    "#### 2. Word Frequency for tweets that are not Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "aedc6f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 2576 examples\n",
      "  84. ADJ  : 3431\n",
      "  85. ADP  : 4746\n",
      "  86. ADV  : 6399\n",
      "  87. AUX  : 3265\n",
      "  89. CCONJ: 1426\n",
      "  90. DET  : 2674\n",
      "  91. INTJ : 98\n",
      "  92. NOUN : 8200\n",
      "  93. NUM  : 449\n",
      "  94. PART : 708\n",
      "  95. PRON : 4896\n",
      "  96. PROPN: 2558\n",
      "  97. PUNCT: 6201\n",
      "  98. SCONJ: 1010\n",
      "  99. SYM  : 262\n",
      " 100. VERB : 5735\n",
      " 101. X    : 554\n",
      " 103. SPACE: 653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bare', 147),\n",
       " ('@USER', 126),\n",
       " ('godt', 123),\n",
       " ('når', 121),\n",
       " ('  ', 117),\n",
       " ('Danmark', 100),\n",
       " ('se', 82),\n",
       " ('helt', 81),\n",
       " ('URL', 77),\n",
       " ('år', 74)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['subtask_a'], ['NOT'], 'tweet', 'da_core_news_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef9f1d",
   "metadata": {},
   "source": [
    "### Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c0466405",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{DATA_DIR}/hindi/agr_hi_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "229685d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['id', 'text', 'agr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dd2097e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 4855 examples\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tweet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\ai_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ai_env\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ai_env\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tweet'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_102440/2350136482.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfreqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_dataframe_and_compute_word_frequency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'agr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'OAG'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tweet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'da_core_news_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfreqs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_102440/1960025340.py\u001b[0m in \u001b[0;36mslice_dataframe_and_compute_word_frequency\u001b[1;34m(df, slice_cols, slice_vals, text_col, spacy_lang_pkg)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Found a total of {len(sliced_df)} examples'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspacy_lang_pkg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msliced_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memoji\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_emoji_regexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ai_env\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ai_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tweet'"
     ]
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['agr'], ['OAG'], 'tweet', 'da_core_news_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English (basile et al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{DATA_DIR}/english-basile/hateval2019_en_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an A...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignori...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text  HS  TR  AG\n",
       "0  201  Hurray, saving us $$$ in so many ways @potus @...   1   0   0\n",
       "1  202  Why would young fighting age men be the vast m...   1   0   0\n",
       "2  203  @KamalaHarris Illegals Dump their Kids at the ...   1   0   0\n",
       "3  204  NY Times: 'Nearly All White' States Pose 'an A...   0   0   0\n",
       "4  205  Orban in Brussels: European leaders are ignori...   0   0   0"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Word Frequency for Examples that are Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 3783 examples\n",
      "  84. ADJ  : 6059\n",
      "  85. ADP  : 7267\n",
      "  86. ADV  : 4209\n",
      "  87. AUX  : 4511\n",
      "  89. CCONJ: 2162\n",
      "  90. DET  : 6672\n",
      "  91. INTJ : 454\n",
      "  92. NOUN : 18766\n",
      "  93. NUM  : 986\n",
      "  94. PART : 2941\n",
      "  95. PRON : 8077\n",
      "  96. PROPN: 10426\n",
      "  97. PUNCT: 9374\n",
      "  98. SCONJ: 1007\n",
      "  99. SYM  : 2719\n",
      " 100. VERB : 12887\n",
      " 101. X    : 382\n",
      " 103. SPACE: 756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bitch', 693),\n",
       " ('women', 336),\n",
       " ('like', 301),\n",
       " ('BuildThatWall', 291),\n",
       " ('refugees', 246),\n",
       " ('illegal', 227),\n",
       " ('whore', 219),\n",
       " ('cunt', 207),\n",
       " ('want', 203),\n",
       " ('migrants', 198)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['HS'], [1], 'text', 'en_core_web_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Word Frequency for Examples that are Hate Speech and Aggressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 1559 examples\n",
      "  84. ADJ  : 2664\n",
      "  85. ADP  : 3336\n",
      "  86. ADV  : 1784\n",
      "  87. AUX  : 1985\n",
      "  89. CCONJ: 1034\n",
      "  90. DET  : 2858\n",
      "  91. INTJ : 206\n",
      "  92. NOUN : 8538\n",
      "  93. NUM  : 463\n",
      "  94. PART : 1276\n",
      "  95. PRON : 3495\n",
      "  96. PROPN: 5230\n",
      "  97. PUNCT: 4462\n",
      "  98. SCONJ: 424\n",
      "  99. SYM  : 1611\n",
      " 100. VERB : 5846\n",
      " 101. X    : 156\n",
      " 103. SPACE: 309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bitch', 220),\n",
       " ('BuildThatWall', 197),\n",
       " ('refugees', 121),\n",
       " ('MAGA', 114),\n",
       " ('illegal', 113),\n",
       " ('like', 107),\n",
       " ('Trump', 102),\n",
       " ('Illegal', 100),\n",
       " ('@realDonaldTrump', 97),\n",
       " ('immigration', 92)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['HS', 'AG'], [1, 1], 'text', 'en_core_web_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Word Frequency for Examples that are Hate Speech but not Aggressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 2224 examples\n",
      "  84. ADJ  : 3396\n",
      "  85. ADP  : 3925\n",
      "  86. ADV  : 2419\n",
      "  87. AUX  : 2535\n",
      "  89. CCONJ: 1128\n",
      "  90. DET  : 3820\n",
      "  91. INTJ : 246\n",
      "  92. NOUN : 10197\n",
      "  93. NUM  : 525\n",
      "  94. PART : 1663\n",
      "  95. PRON : 4581\n",
      "  96. PROPN: 5208\n",
      "  97. PUNCT: 4924\n",
      "  98. SCONJ: 583\n",
      "  99. SYM  : 1131\n",
      " 100. VERB : 7040\n",
      " 101. X    : 210\n",
      " 103. SPACE: 447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bitch', 473),\n",
       " ('women', 258),\n",
       " ('like', 194),\n",
       " ('whore', 152),\n",
       " ('woman', 147),\n",
       " ('hoe', 144),\n",
       " ('cunt', 140),\n",
       " ('refugees', 125),\n",
       " ('ass', 118),\n",
       " ('want', 117)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['HS', 'AG'], [1, 0], 'text', 'en_core_web_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Frequency for Examples that are not Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 5217 examples\n",
      "  84. ADJ  : 7958\n",
      "  85. ADP  : 10639\n",
      "  86. ADV  : 5168\n",
      "  87. AUX  : 5378\n",
      "  89. CCONJ: 2961\n",
      "  90. DET  : 8299\n",
      "  91. INTJ : 570\n",
      "  92. NOUN : 24157\n",
      "  93. NUM  : 1639\n",
      "  94. PART : 3843\n",
      "  95. PRON : 9613\n",
      "  96. PROPN: 13798\n",
      "  97. PUNCT: 12669\n",
      "  98. SCONJ: 1239\n",
      "  99. SYM  : 1816\n",
      " 100. VERB : 15777\n",
      " 101. X    : 504\n",
      " 103. SPACE: 1015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('refugees', 557),\n",
       " ('immigrant', 509),\n",
       " ('women', 405),\n",
       " ('men', 377),\n",
       " ('like', 332),\n",
       " ('migrants', 292),\n",
       " ('people', 259),\n",
       " ('rape', 234),\n",
       " ('refugee', 233),\n",
       " ('woman', 231)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['HS'], [0], 'text', 'en_core_web_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English (ousidhoum et al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{DATA_DIR}/english-ousidhoum/en_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HITId</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>directness</th>\n",
       "      <th>annotator_sentiment</th>\n",
       "      <th>target</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>call sis im boy girl still faggot shut</td>\n",
       "      <td>offensive</td>\n",
       "      <td>indirect</td>\n",
       "      <td>confusion_disgust</td>\n",
       "      <td>origin</td>\n",
       "      <td>gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@user @user legit nilas retarded idk</td>\n",
       "      <td>offensive_disrespectful</td>\n",
       "      <td>indirect</td>\n",
       "      <td>anger_confusion</td>\n",
       "      <td>disability</td>\n",
       "      <td>special_needs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>said retard @url</td>\n",
       "      <td>fearful_abusive_offensive_hateful</td>\n",
       "      <td>indirect</td>\n",
       "      <td>confusion_disgust</td>\n",
       "      <td>disability</td>\n",
       "      <td>special_needs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>america another 8 years obama's ideology via h...</td>\n",
       "      <td>fearful_abusive_hateful_disrespectful_normal</td>\n",
       "      <td>indirect</td>\n",
       "      <td>anger_fear_shock_sadness_disgust</td>\n",
       "      <td>origin</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@user don... fucking made cry twat.</td>\n",
       "      <td>offensive</td>\n",
       "      <td>indirect</td>\n",
       "      <td>shock_disgust</td>\n",
       "      <td>gender</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HITId                                              tweet  \\\n",
       "0      1             call sis im boy girl still faggot shut   \n",
       "1      2               @user @user legit nilas retarded idk   \n",
       "2      3                                   said retard @url   \n",
       "3      4  america another 8 years obama's ideology via h...   \n",
       "4      5                @user don... fucking made cry twat.   \n",
       "\n",
       "                                      sentiment directness  \\\n",
       "0                                     offensive   indirect   \n",
       "1                       offensive_disrespectful   indirect   \n",
       "2             fearful_abusive_offensive_hateful   indirect   \n",
       "3  fearful_abusive_hateful_disrespectful_normal   indirect   \n",
       "4                                     offensive   indirect   \n",
       "\n",
       "                annotator_sentiment      target          group  \n",
       "0                 confusion_disgust      origin            gay  \n",
       "1                   anger_confusion  disability  special_needs  \n",
       "2                 confusion_disgust  disability  special_needs  \n",
       "3  anger_fear_shock_sadness_disgust      origin          other  \n",
       "4                     shock_disgust      gender          women  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Word Frequency for Examples that are not Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 661 examples\n",
      "  84. ADJ  : 784\n",
      "  85. ADP  : 121\n",
      "  86. ADV  : 275\n",
      "  87. AUX  : 166\n",
      "  89. CCONJ: 29\n",
      "  90. DET  : 125\n",
      "  91. INTJ : 54\n",
      "  92. NOUN : 2104\n",
      "  93. NUM  : 107\n",
      "  94. PART : 73\n",
      "  95. PRON : 195\n",
      "  96. PROPN: 633\n",
      "  97. PUNCT: 558\n",
      "  98. SCONJ: 10\n",
      "  99. SYM  : 72\n",
      " 100. VERB : 1318\n",
      " 101. X    : 360\n",
      " 103. SPACE: 45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('@user', 396),\n",
       " ('@url', 304),\n",
       " ('retarded', 62),\n",
       " ('shithole', 61),\n",
       " ('like', 54),\n",
       " ('faggot', 51),\n",
       " ('spic', 49),\n",
       " ('retard', 45),\n",
       " ('twat', 42),\n",
       " ('cunt', 39)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['normal'], 'tweet', 'en_core_web_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Word Frequency for Examples that are Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 2954 examples\n",
      "  84. ADJ  : 3441\n",
      "  85. ADP  : 379\n",
      "  86. ADV  : 1181\n",
      "  87. AUX  : 720\n",
      "  89. CCONJ: 124\n",
      "  90. DET  : 417\n",
      "  91. INTJ : 261\n",
      "  92. NOUN : 8210\n",
      "  93. NUM  : 338\n",
      "  94. PART : 264\n",
      "  95. PRON : 776\n",
      "  96. PROPN: 2588\n",
      "  97. PUNCT: 2408\n",
      "  98. SCONJ: 42\n",
      "  99. SYM  : 178\n",
      " 100. VERB : 5376\n",
      " 101. X    : 2131\n",
      " 103. SPACE: 208\n",
      "Found a total of 1 examples\n",
      "  86. ADV  : 3\n",
      "  87. AUX  : 1\n",
      "  92. NOUN : 1\n",
      "  95. PRON : 1\n",
      "  96. PROPN: 2\n",
      "  97. PUNCT: 2\n",
      " 100. VERB : 2\n",
      "Found a total of 315 examples\n",
      "  84. ADJ  : 359\n",
      "  85. ADP  : 35\n",
      "  86. ADV  : 149\n",
      "  87. AUX  : 80\n",
      "  89. CCONJ: 12\n",
      "  90. DET  : 45\n",
      "  91. INTJ : 22\n",
      "  92. NOUN : 930\n",
      "  93. NUM  : 30\n",
      "  94. PART : 37\n",
      "  95. PRON : 102\n",
      "  96. PROPN: 283\n",
      "  97. PUNCT: 283\n",
      "  98. SCONJ: 8\n",
      "  99. SYM  : 20\n",
      " 100. VERB : 586\n",
      " 101. X    : 172\n",
      " 103. SPACE: 13\n",
      "Found a total of 10 examples\n",
      "  84. ADJ  : 9\n",
      "  85. ADP  : 1\n",
      "  86. ADV  : 3\n",
      "  87. AUX  : 1\n",
      "  91. INTJ : 1\n",
      "  92. NOUN : 31\n",
      "  94. PART : 1\n",
      "  95. PRON : 2\n",
      "  96. PROPN: 13\n",
      "  97. PUNCT: 2\n",
      "  99. SYM  : 1\n",
      " 100. VERB : 16\n",
      " 101. X    : 5\n",
      " 103. SPACE: 4\n",
      "Found a total of 28 examples\n",
      "  84. ADJ  : 32\n",
      "  85. ADP  : 2\n",
      "  86. ADV  : 11\n",
      "  87. AUX  : 4\n",
      "  90. DET  : 6\n",
      "  91. INTJ : 4\n",
      "  92. NOUN : 92\n",
      "  93. NUM  : 5\n",
      "  94. PART : 2\n",
      "  95. PRON : 5\n",
      "  96. PROPN: 22\n",
      "  97. PUNCT: 28\n",
      "  99. SYM  : 1\n",
      " 100. VERB : 57\n",
      " 101. X    : 11\n",
      " 103. SPACE: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('@user', 2787),\n",
       " ('@url', 1340),\n",
       " ('retard', 406),\n",
       " ('retarded', 372),\n",
       " ('faggot', 324),\n",
       " ('cunt', 287),\n",
       " ('shithole', 283),\n",
       " ('twat', 280),\n",
       " ('like', 220),\n",
       " ('fucking', 207)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['offensive'], 'tweet', 'en_core_web_sm')\n",
    "freqs += slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['abusive'], 'tweet', 'en_core_web_sm')\n",
    "freqs += slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['hateful'], 'tweet', 'en_core_web_sm')\n",
    "freqs += slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['disrespectful'], 'tweet', 'en_core_web_sm')\n",
    "freqs += slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['fearful'], 'tweet', 'en_core_web_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{DATA_DIR}/arabic-ousidhoum/ar_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HITId</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>directness</th>\n",
       "      <th>annotator_sentiment</th>\n",
       "      <th>target</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>صلاة الفجر خير لك من ترديد بول البعير وسبي الن...</td>\n",
       "      <td>hateful_normal</td>\n",
       "      <td>indirect</td>\n",
       "      <td>shock</td>\n",
       "      <td>gender</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>صراحة نفسي اشوف ولاد الوسخة اللي قالوا مدرب اج...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>indirect</td>\n",
       "      <td>anger_confusion_sadness_indifference_disgust</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>طيب! هي متبرجة وعبايتها ملونه وطالعة من بيتهم ...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>indirect</td>\n",
       "      <td>indifference</td>\n",
       "      <td>other</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@user @user انا اوافقك بخصوص السوريين و العراق...</td>\n",
       "      <td>normal</td>\n",
       "      <td>direct</td>\n",
       "      <td>indifference</td>\n",
       "      <td>origin</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>هذه السعودية التي شعبها شعب الخيم و بول البعير...</td>\n",
       "      <td>normal</td>\n",
       "      <td>indirect</td>\n",
       "      <td>indifference</td>\n",
       "      <td>origin</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HITId                                              tweet       sentiment  \\\n",
       "0      1  صلاة الفجر خير لك من ترديد بول البعير وسبي الن...  hateful_normal   \n",
       "1      2  صراحة نفسي اشوف ولاد الوسخة اللي قالوا مدرب اج...       offensive   \n",
       "2      3  طيب! هي متبرجة وعبايتها ملونه وطالعة من بيتهم ...       offensive   \n",
       "3      4  @user @user انا اوافقك بخصوص السوريين و العراق...          normal   \n",
       "4      5  هذه السعودية التي شعبها شعب الخيم و بول البعير...          normal   \n",
       "\n",
       "  directness                           annotator_sentiment  target       group  \n",
       "0   indirect                                         shock  gender  individual  \n",
       "1   indirect  anger_confusion_sadness_indifference_disgust   other       other  \n",
       "2   indirect                                  indifference   other  individual  \n",
       "3     direct                                  indifference  origin       other  \n",
       "4   indirect                                  indifference  origin       other  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Word Frequency for Examples that are not Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 915 examples\n",
      "  84. ADJ  : 532\n",
      "  85. ADP  : 254\n",
      "  86. ADV  : 191\n",
      "  87. AUX  : 14\n",
      "  89. CCONJ: 13\n",
      "  90. DET  : 91\n",
      "  91. INTJ : 41\n",
      "  92. NOUN : 2154\n",
      "  93. NUM  : 52\n",
      "  94. PART : 3\n",
      "  95. PRON : 82\n",
      "  96. PROPN: 7300\n",
      "  97. PUNCT: 715\n",
      "  99. SYM  : 54\n",
      " 100. VERB : 1058\n",
      " 101. X    : 603\n",
      " 103. SPACE: 142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('@user', 529),\n",
       " ('@url', 336),\n",
       " ('في', 226),\n",
       " ('من', 218),\n",
       " ('التحرش', 214),\n",
       " ('الحريم', 119),\n",
       " ('على', 105),\n",
       " ('البعير', 93),\n",
       " ('بول', 91),\n",
       " ('ما', 77)]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['normal'], 'tweet', 'en_core_web_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Word Frequency for Examples that are Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 950 examples\n",
      "  84. ADJ  : 570\n",
      "  85. ADP  : 204\n",
      "  86. ADV  : 222\n",
      "  87. AUX  : 14\n",
      "  89. CCONJ: 10\n",
      "  90. DET  : 93\n",
      "  91. INTJ : 46\n",
      "  92. NOUN : 2255\n",
      "  93. NUM  : 31\n",
      "  94. PART : 3\n",
      "  95. PRON : 78\n",
      "  96. PROPN: 6650\n",
      "  97. PUNCT: 614\n",
      "  99. SYM  : 49\n",
      " 100. VERB : 1133\n",
      " 101. X    : 1007\n",
      " 103. SPACE: 146\n",
      "Found a total of 19 examples\n",
      "  84. ADJ  : 12\n",
      "  85. ADP  : 5\n",
      "  86. ADV  : 4\n",
      "  90. DET  : 2\n",
      "  92. NOUN : 67\n",
      "  93. NUM  : 4\n",
      "  96. PROPN: 147\n",
      "  97. PUNCT: 13\n",
      "  99. SYM  : 2\n",
      " 100. VERB : 24\n",
      " 101. X    : 44\n",
      "Found a total of 460 examples\n",
      "  84. ADJ  : 314\n",
      "  85. ADP  : 136\n",
      "  86. ADV  : 118\n",
      "  87. AUX  : 10\n",
      "  89. CCONJ: 2\n",
      "  90. DET  : 31\n",
      "  91. INTJ : 20\n",
      "  92. NOUN : 1203\n",
      "  93. NUM  : 23\n",
      "  95. PRON : 33\n",
      "  96. PROPN: 3377\n",
      "  97. PUNCT: 320\n",
      "  99. SYM  : 14\n",
      " 100. VERB : 581\n",
      " 101. X    : 634\n",
      " 103. SPACE: 98\n",
      "Found a total of 167 examples\n",
      "  84. ADJ  : 72\n",
      "  85. ADP  : 29\n",
      "  86. ADV  : 52\n",
      "  87. AUX  : 3\n",
      "  90. DET  : 15\n",
      "  91. INTJ : 8\n",
      "  92. NOUN : 322\n",
      "  93. NUM  : 3\n",
      "  95. PRON : 6\n",
      "  96. PROPN: 966\n",
      "  97. PUNCT: 73\n",
      "  99. SYM  : 6\n",
      " 100. VERB : 199\n",
      " 101. X    : 218\n",
      " 103. SPACE: 29\n",
      "Found a total of 12 examples\n",
      "  84. ADJ  : 8\n",
      "  85. ADP  : 8\n",
      "  86. ADV  : 5\n",
      "  87. AUX  : 1\n",
      "  90. DET  : 3\n",
      "  91. INTJ : 2\n",
      "  92. NOUN : 41\n",
      "  93. NUM  : 1\n",
      "  95. PRON : 1\n",
      "  96. PROPN: 108\n",
      "  97. PUNCT: 6\n",
      " 100. VERB : 15\n",
      " 101. X    : 15\n",
      " 103. SPACE: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('@user', 1984),\n",
       " ('@url', 488),\n",
       " ('بول', 402),\n",
       " ('البعير', 389),\n",
       " ('من', 374),\n",
       " ('يا', 374),\n",
       " ('خنازير', 342),\n",
       " ('خنزير', 248),\n",
       " ('في', 213),\n",
       " ('على', 161)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['offensive'], 'tweet', 'en_core_web_sm')\n",
    "freqs += slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['abusive'], 'tweet', 'en_core_web_sm')\n",
    "freqs += slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['hateful'], 'tweet', 'en_core_web_sm')\n",
    "freqs += slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['disrespectful'], 'tweet', 'en_core_web_sm')\n",
    "freqs += slice_dataframe_and_compute_word_frequency(df, ['sentiment'], ['fearful'], 'tweet', 'en_core_web_sm')\n",
    "freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b9210e76c871583b4be282bfe8fe4a35092a039f09ad1ca55e305e87262e8ac"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ai_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
