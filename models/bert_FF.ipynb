{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "MAX_TOKEN_COUNT = 512\n",
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "TEXT_COLUMN_NAME = 'text'\n",
    "LABEL_COLUMN_NAME = 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv('../data/kaggle/Fake.csv')\n",
    "real = pd.read_csv('../data/kaggle/True.csv')\n",
    "fake = fake[fake['text'] != ' ']\n",
    "fake = fake[fake['text'] != '  ']\n",
    "real = real[real['text'] != ' ']\n",
    "real = real[real['text'] != '  ']\n",
    "\n",
    "publisher = [] #storing information for publisher (new column will be created after)\n",
    "tmp_text = [] #temporal text\n",
    "\n",
    "unknown_publishers = []\n",
    "for index, row in enumerate(real.text.values):\n",
    "    try:\n",
    "        record = row.split('-', maxsplit=1) #there is a dash dividing the text (at the text column)\n",
    "        record[1] #second part of the text\n",
    "        \n",
    "        assert(len(record[0])<120) #checking if #chars is less tham 120\n",
    "    except:\n",
    "        unknown_publishers.append(index)\n",
    "\n",
    "for index, row in enumerate(real.text.values): #Including unknown publishers\n",
    "    if index in unknown_publishers:\n",
    "        tmp_text.append(row)\n",
    "        publisher.append('Unknown')\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        record = row.split('-', maxsplit = 1)\n",
    "        publisher.append(record[0].strip())\n",
    "        tmp_text.append(record[1].strip())\n",
    "real['publisher'] = publisher #replacing columns by new content\n",
    "real['text'] = tmp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, text, subject, date, publisher]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_real_index = [index for index, text in enumerate(real.text.tolist()) if str(text).strip()==\"\"] \n",
    "real.iloc[empty_real_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, text, subject, date]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_fake_index = [index for index, text in enumerate(fake.text.tolist()) if str(text).strip()==\"\"] \n",
    "fake.iloc[empty_fake_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "real['text'] = real['title'] + \" \" + real['text'] #Putting title and text in the same column\n",
    "fake['text'] = fake['title'] + \" \" + fake['text']\n",
    "real['text'] = real['text'].apply(lambda x: str(x).lower()) #converting to lower case\n",
    "fake['text'] = fake['text'].apply(lambda x: str(x).lower())\n",
    "real['class'] = 1.0 #Adding a new column, indicating it is true (1) or false (0) \n",
    "fake['class'] = 0.0\n",
    "real = real[['text', 'class']] #We will just keep text and class (label)\n",
    "fake = fake[['text', 'class']] #We will just keep text and class (label)\n",
    "df = real.append(fake, ignore_index=True) #Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as u.s. budget fight looms, republicans flip t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u.s. military to accept transgender recruits o...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>senior u.s. republican senator: 'let mr. muell...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fbi russia probe helped by australian diplomat...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trump wants postal service to charge 'much mor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44262</th>\n",
       "      <td>mcpain: john mccain furious that iran treated ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44263</th>\n",
       "      <td>justice? yahoo settles e-mail privacy class-ac...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44264</th>\n",
       "      <td>sunnistan: us and allied ‘safe zone’ plan to t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44265</th>\n",
       "      <td>how to blow $700 million: al jazeera america f...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44266</th>\n",
       "      <td>10 u.s. navy sailors held by iranian military ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44267 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  class\n",
       "0      as u.s. budget fight looms, republicans flip t...    1.0\n",
       "1      u.s. military to accept transgender recruits o...    1.0\n",
       "2      senior u.s. republican senator: 'let mr. muell...    1.0\n",
       "3      fbi russia probe helped by australian diplomat...    1.0\n",
       "4      trump wants postal service to charge 'much mor...    1.0\n",
       "...                                                  ...    ...\n",
       "44262  mcpain: john mccain furious that iran treated ...    0.0\n",
       "44263  justice? yahoo settles e-mail privacy class-ac...    0.0\n",
       "44264  sunnistan: us and allied ‘safe zone’ plan to t...    0.0\n",
       "44265  how to blow $700 million: al jazeera america f...    0.0\n",
       "44266  10 u.s. navy sailors held by iranian military ...    0.0\n",
       "\n",
       "[44267 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    22851\n",
       "1.0    21416\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    44267.000000\n",
       "mean       432.100052\n",
       "std        361.228903\n",
       "min          2.000000\n",
       "25%        229.000000\n",
       "50%        385.000000\n",
       "75%        541.000000\n",
       "max       8449.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].map(\n",
    "    lambda text: len(str(text).split(\" \"))\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42053, 2), (2214, 2))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.05)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as u.s. budget fight looms, republicans flip their fiscal script the head of a conservative republican faction in the u.s. congress, who voted this month for a huge expansion of the national debt to pay for tax cuts, called himself a “fiscal conservative” on sunday and urged budget restraint in 2018. in keeping with a sharp pivot under way among republicans, u.s. representative mark meadows, speaking on cbs’ “face the nation,” drew a hard line on federal spending, which lawmakers are bracing to do battle over in january. when they return from the holidays on wednesday, lawmakers will begin trying to pass a federal budget in a fight likely to be linked to other issues, such as immigration policy, even as the november congressional election campaigns approach in which republicans will seek to keep control of congress. president donald trump and his republicans want a big budget increase in military spending, while democrats also want proportional increases for non-defense “discretionary” spending on programs that support education, scientific research, infrastructure, public health and environmental protection. “the (trump) administration has already been willing to say: ‘we’re going to increase non-defense discretionary spending ... by about 7 percent,’” meadows, chairman of the small but influential house freedom caucus, said on the program. “now, democrats are saying that’s not enough, we need to give the government a pay raise of 10 to 11 percent. for a fiscal conservative, i don’t see where the rationale is. ... eventually you run out of other people’s money,” he said. meadows was among republicans who voted in late december for their party’s debt-financed tax overhaul, which is expected to balloon the federal budget deficit and add about $1.5 trillion over 10 years to the $20 trillion national debt. “it’s interesting to hear mark talk about fiscal responsibility,” democratic u.s. representative joseph crowley said on cbs. crowley said the republican tax bill would require the  united states to borrow $1.5 trillion, to be paid off by future generations, to finance tax cuts for corporations and the rich. “this is one of the least ... fiscally responsible bills we’ve ever seen passed in the history of the house of representatives. i think we’re going to be paying for this for many, many years to come,” crowley said. republicans insist the tax package, the biggest u.s. tax overhaul in more than 30 years,  will boost the economy and job growth. house speaker paul ryan, who also supported the tax bill, recently went further than meadows, making clear in a radio interview that welfare or “entitlement reform,” as the party often calls it, would be a top republican priority in 2018. in republican parlance, “entitlement” programs mean food stamps, housing assistance, medicare and medicaid health insurance for the elderly, poor and disabled, as well as other programs created by washington to assist the needy. democrats seized on ryan’s early december remarks, saying they showed republicans would try to pay for their tax overhaul by seeking spending cuts for social programs. but the goals of house republicans may have to take a back seat to the senate, where the votes of some democrats will be needed to approve a budget and prevent a government shutdown. democrats will use their leverage in the senate, which republicans narrowly control, to defend both discretionary non-defense programs and social spending, while tackling the issue of the “dreamers,” people brought illegally to the country as children. trump in september put a march 2018 expiration date on the deferred action for childhood arrivals, or daca, program, which protects the young immigrants from deportation and provides them with work permits. the president has said in recent twitter messages he wants funding for his proposed mexican border wall and other immigration law changes in exchange for agreeing to help the dreamers. representative debbie dingell told cbs she did not favor linking that issue to other policy objectives, such as wall funding. “we need to do daca clean,” she said.  on wednesday, trump aides will meet with congressional leaders to discuss those issues. that will be followed by a weekend of strategy sessions for trump and republican leaders on jan. 6 and 7, the white house said. trump was also scheduled to meet on sunday with florida republican governor rick scott, who wants more emergency aid. the house has passed an $81 billion aid package after hurricanes in florida, texas and puerto rico, and wildfires in california. the package far exceeded the $44 billion requested by the trump administration. the senate has not yet voted on the aid.\n"
     ]
    }
   ],
   "source": [
    "sample_row = df.iloc[0]\n",
    "sample_comment = sample_row['text']\n",
    "sample_labels = sample_row['class']\n",
    "\n",
    "print(sample_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: BertTokenizer,\n",
    "        max_token_len: int = MAX_TOKEN_COUNT\n",
    "    ):\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        text = data_row[TEXT_COLUMN_NAME]\n",
    "        labels = data_row[LABEL_COLUMN_NAME]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return dict(\n",
    "            input_ids=encoding[\"input_ids\"].flatten(),\n",
    "            attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "            labels=labels\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FakeNewsDataset(\n",
    "  train_df,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "data_loader1 = DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FakeNewsDataset(\n",
    "  train_df,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "sample_item = train_dataset[0]\n",
    "# sample_item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_item[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_item[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_item[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_item[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n",
    "# output = bert(sample_item[\"input_ids\"], attention_mask=sample_item[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_df, test_df, tokenizer, batch_size=8, max_token_len=128):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = FakeNewsDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "\n",
    "        self.test_dataset = FakeNewsDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    # def train_dataloader(self):\n",
    "    #     return DataLoader(\n",
    "    #         self.train_dataset,\n",
    "    #         batch_size=self.batch_size,\n",
    "    #         shuffle=True,\n",
    "    #         num_workers=1\n",
    "    #     )\n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     return DataLoader(\n",
    "    #         self.test_dataset,\n",
    "    #         batch_size=self.batch_size,\n",
    "    #         num_workers=1\n",
    "    #     )\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(\n",
    "    #         self.test_dataset,\n",
    "    #         batch_size=self.batch_size,\n",
    "    #         num_workers=1\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "data_module = FakeNewsDataModule(\n",
    "  train_df,\n",
    "  val_df,\n",
    "  tokenizer,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "class FakeNewsTagger(pl.LightningModule):\n",
    "    def __init__(self, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n",
    "        self.i2h = nn.Linear(self.bert.config.hidden_size, 128)\n",
    "        self.h2o = nn.Linear(128, 1)\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.auroc = torchmetrics.AUROC()\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        output = self.i2h(output.pooler_output)\n",
    "        output = self.h2o(output)\n",
    "        output = torch.sigmoid(output).flatten()\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output.to(dtype=float), labels.to(dtype=float))\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True, batch_size=BATCH_SIZE)\n",
    "        # self.log(\"train_AUROC\", self.auroc(outputs, labels), prog_bar=True, logger=True, batch_size=BATCH_SIZE)\n",
    "        self.log(\"train_acc\", self.calculate_accuracy(outputs, labels), prog_bar=True, logger=True, batch_size=BATCH_SIZE)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True, batch_size=BATCH_SIZE)\n",
    "        # self.log(\"val_AUROC\", self.auroc(outputs, labels), prog_bar=True, logger=True, batch_size=BATCH_SIZE)\n",
    "        self.log(\"val_acc\", self.calculate_accuracy(outputs, labels), prog_bar=True, logger=True, batch_size=BATCH_SIZE)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True, batch_size=BATCH_SIZE)\n",
    "        # self.log(\"test_AUROC\", self.auroc(outputs, labels), prog_bar=True, logger=True, batch_size=BATCH_SIZE)\n",
    "        self.log(\"test_acc\", self.calculate_accuracy(outputs, labels), prog_bar=True, logger=True, batch_size=BATCH_SIZE)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for output in outputs:\n",
    "            for out_labels in output[\"labels\"].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "                predictions.append(out_predictions)\n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "        roc_auc = self.auroc(predictions, labels)\n",
    "        self.logger.experiment.add_scalar(\"roc_auc/Train\", roc_auc, self.current_epoch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def calculate_auroc(self, outputs, labels):\n",
    "        return self.auroc(torch.round(outputs).to(int), labels.to(int))\n",
    "\n",
    "    def calculate_accuracy(self, outputs, labels):\n",
    "        return self.accuracy(torch.round(outputs).to(int), labels.to(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\Nicholas\\.conda\\envs\\rts\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = FakeNewsTagger(\n",
    "  n_warmup_steps=20,\n",
    "  n_training_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath=\"checkpoints\",\n",
    "  filename=\"best-checkpoint\",\n",
    "  save_top_k=1,\n",
    "  verbose=True,\n",
    "  monitor=\"val_loss\",\n",
    "  mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"fake-news-ff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\.conda\\envs\\rts\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x000001B03ECD3A48>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x000001B03ECD3A48>)`.\n",
      "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
      "C:\\Users\\Nicholas\\.conda\\envs\\rts\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=30)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "  logger=logger,\n",
    "  checkpoint_callback=checkpoint_callback,\n",
    "  callbacks=[early_stopping_callback],\n",
    "  max_epochs=10,\n",
    "  gpus=1,\n",
    "  progress_bar_refresh_rate=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | bert      | BertModel | 108 M \n",
      "1 | i2h       | Linear    | 98.4 K\n",
      "2 | h2o       | Linear    | 129   \n",
      "3 | criterion | BCELoss   | 0     \n",
      "4 | auroc     | AUROC     | 0     \n",
      "5 | accuracy  | Accuracy  | 0     \n",
      "----------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "433.635   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\.conda\\envs\\rts\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:111: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\.conda\\envs\\rts\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2768 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\.conda\\envs\\rts\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\closure.py:36: LightningDeprecationWarning: One of the returned values {'labels', 'predictions'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n",
      "  f\"One of the returned values {set(extra.keys())} has a `grad_fn`. We will detach it automatically\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2768/2768 [18:10<00:00,  2.54it/s, loss=0.136, v_num=51, train_loss=0.0583, train_acc=1.000, val_loss=0.165, val_acc=0.949]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)\n",
    "# trainer.fit(model, data_loader1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Nicholas\\.conda\\envs\\rts\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:111: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 139/139 [00:20<00:00,  6.72it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9487410187721252, 'test_loss': 0.16516122221946716}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 139/139 [00:20<00:00,  6.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.16516122221946716, 'test_acc': 0.9487410187721252}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint = torch.load('lightning_logs/fake-news-snopes/version_40/checkpoints/epoch=2-step=7886.ckpt')\n",
    "# model.load_from_checkpoint('lightning_logs/fake-news-snopes/version_40/checkpoints/epoch=2-step=7886.ckpt')\n",
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b9210e76c871583b4be282bfe8fe4a35092a039f09ad1ca55e305e87262e8ac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
